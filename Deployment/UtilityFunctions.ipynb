{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UtilityFunctions.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4st0uqXwrPC4qQ8kPPLkl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HHVhgPu5z4x5"},"outputs":[],"source":["''' Task: Data pre-processing'''\n","\n","def preProcessData(eegdata, dgdata):\n","    '''eegdata.SubjectID=eegdata.SubjectID.astype(int)\n","    eegdata.VideoID=eegdata.VideoID.astype(int)\n","    eegdata.predefinedlabel=eegdata.predefinedlabel.astype(int)\n","    eegdata['user-definedlabeln']=eegdata['user-definedlabeln'].astype(int)\n","\n","    eegdata.rename(columns = {'predefinedlabel':'TgtLabel', 'user-definedlabeln':'UsrTgtLabel'}, inplace = True)\n","    demodata.rename(columns={'subject ID':'SubjectID'}, inplace=True)\n","    #eegdata=eegdata.merge(dgInfo,'inner',left_on='SubjectID', right_on='subject ID')\n","    \n","    eegdata=pd.merge(eegdata,demodata,how='inner',on='SubjectID')\n","\n","    eegdata=pd.get_dummies(eegdata)\n","\n","\n","    #Rename columns and Create id column as unique row identifier\n","    eegdata.rename(columns={'SubjectID':'SubjectId'}, inplace=True)\n","    eegdata.rename(columns={'VideoID':'VideoId'}, inplace=True)\n","    demodict={' age': 'Age', ' ethnicity_Bengali' : 'EthnicityBengali',\n","       ' ethnicity_English' : 'EthnicityEnglish', ' ethnicity_Han Chinese' : 'EthnicityHanChinese', \n","       ' gender_F' : 'Female', ' gender_M' : 'Male'}\n","    eegdata.rename(columns=demodict, inplace=True)\n","\n","    eegdata['SubVdId']=eegdata['SubjectId'].map(str)+'-'+eegdata['VideoId'].map(str)\n","    eegdata['TimeSecs']=eegdata.groupby(['SubjectId','VideoId']).cumcount()+1\n","    videoLength=eegdata[['SubjectId', 'VideoId']].value_counts().to_list()\n","    eegdata=eegdata[eegdata.TimeSecs<=112]'''\n","\n","\n","    \n","\n","    eegdata.SubjectID=eegdata.SubjectID.astype(int)\n","    eegdata.VideoID=eegdata.VideoID.astype(int)\n","    eegdata.predefinedlabel=eegdata.predefinedlabel.astype(int)\n","    eegdata['user-definedlabeln']=eegdata['user-definedlabeln'].astype(int)\n","\n","    eegdata.rename(columns = {'predefinedlabel':'TgtLabel', 'user-definedlabeln':'UsrTgtLabel',\n","                          'SubjectID':'SubjectId', 'VideoID':'VideoId'}, inplace = True)\n","\n","    eegdata['SubVdId']=eegdata['SubjectId'].map(str)+'-'+eegdata['VideoId'].map(str)\n","    eegdata['TimeSecs']=eegdata.groupby(['SubjectId','VideoId']).cumcount()+1\n","    videoLength=eegdata[['SubjectId', 'VideoId']].value_counts().to_list()\n","    eegdata=eegdata[eegdata.TimeSecs<=112]\n","\n","\n","    demodict={'age': 'Age', 'ethnicity_Bengali' : 'EthnicityBengali',\n","       'ethnicity_English' : 'EthnicityEnglish', 'ethnicity_Han Chinese' : 'EthnicityHanChinese', \n","       'gender_F' : 'Female', 'gender_M' : 'Male', 'subject ID':'SubjectId'}\n","    dgdata.rename(columns=demodict, inplace=True)\n","    dgdata=pd.get_dummies(dgdata)\n","\n","    return eegdata, dgdata, videoLength"]},{"cell_type":"code","source":["'''Task - Create datasets for multiple event epochs - 4,5,6,7,8,9,10,12,15, for any secs segements.\n","Note - Ensure to pass the appropriate event segment paramater value - epochSize'''\n","\n","def fftFeatures(signal, topValues):\n","    fft=np.fft.fft(signal) #Computes the FFT\n","    magSpect=np.round_(np.abs(fft),2) # Extracts the absolute part from the complex numbers\n","    \n","    sortMag=magSpect[np.argsort(magSpect)] #Sorts the magnitude\n","    uniMag=np.unique(sortMag)[-topValues:] #Extract top unique values \n","    \n","    inter, indMag, indUni=np.intersect1d(magSpect,uniMag, return_indices=True) #Gets the timestep index at which these top values occured\n","    #eventInd=np.flip(indMag) #np.sort(indMag) #old logic\n","    eventInd=list(range(len(indMag)))\n","\n","    for n in range(len(indMag)):\n","        num=indMag[n]\n","        if num==0:\n","            eventInd[n]=0\n","        else:\n","            eventInd[n]=np.round(float(1/num),2)\n","    \n","    eventInd=np.flip(eventInd)\n","\n","    return uniMag, eventInd #Returnd the unique top Magnitudes and the respective timestep at which this event magnitude occured"],"metadata":{"id":"7sqsz-XG2rcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def genStatFFTFeatures(dataSub, id, epochSize=15):\n","    #Initialize variables and data structures\n","    st=0\n","    end=len(dataSub)\n","\n","    dat=[]\n","    lbl=[]\n","    #Process from the beginning to the end of the trial\n","    for e in [x for x in range(end) if x%epochSize==0]:\n","    \n","        '''Generate statistical features with the time segment attached as part of the new column generated \n","        after every epochSize per Subject'''\n","        dataStat=dataSub[['SubVdId', 'Delta','Theta','Alpha1','Alpha2','Beta1','Beta2','Gamma1','Gamma2']][e:e+epochSize]\n","        dataStat=dataStat.groupby(['SubVdId']).agg(['mean','std','var','median','min','max', 'skew'])\n","        dataStat.columns = [''.join(str(i) for i in col) for col in dataStat.columns]\n","        nameIndex=e+epochSize\n","        dataStat.columns=[col+'_'+str(nameIndex) for col in dataStat.columns]\n","        lbl.append(dataStat.columns)\n","        dat.append(dataStat.values)\n","\n","    #Create a dataframe by combining the generated column names and statistical features\n","    result=pd.DataFrame(data=np.concatenate(dat).ravel(), columns=['val'])\n","    result['rowIndex']=[item for elem in lbl for item in elem]\n","\n","    magnitude, magnitudeInd=fftFeatures(dataSub['Raw'], topValues=topFFT)\n","    mag=[]\n","    magInd=[]\n","\n","    for m in range(0,len(magnitude)):\n","        colMag='FFTMag_'+str(m)\n","        mag.append(colMag)\n","\n","    for i in range(0,len(magnitudeInd)):\n","        colMagInd='EventMag_'+str(i)\n","        magInd.append(colMagInd)\n","    \n","    dictKey=[*magnitude, *magnitudeInd]\n","    dictVal=[*mag, *magInd]\n","    magDict={'val':dictKey, 'rowIndex':dictVal} #magDict={'val':magnitude, 'colnm':mag}\n","    fftDf=pd.DataFrame(magDict)\n","    result=result.append(fftDf)\n","      \n","    result=pd.pivot_table(result, columns='rowIndex', values='val')\n","    return result\n"],"metadata":{"id":"AI9YUetX2uXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Input data preparation\n","def genFeatures(inpdata):\n","    eegFeatureSet=pd.DataFrame()\n","    svId=inpdata['SubVdId'].unique()\n","    magDict={}\n","\n","    '''Loop through to generate features for all the Subjects.\n","    Remember to pass the appropriate epochSize here in the function call'''\n","    for s in svId:\n","        svDf=inpdata[inpdata['SubVdId']==s]\n","        eegFeatureSet=eegFeatureSet.append(genStatFFTFeatures(dataSub=svDf, id=s, epochSize=eventMarker))\n","\n","    '''Create row index for easier identification of the subject combinations.\n","    The videos presented to subjects are of varying length. \n","    Hence for certain shorter video id combinations like 120secs certain features at 140 secs cannot be generated.\n","    Substitute that with median of the respective feature (not 0) else compute the statistical features till 140+secs'''\n","    eegFeatureSet.index=inpdata['SubVdId'].unique()\n","\n","\n","    for n in eegFeatureSet.columns[eegFeatureSet.isnull().any(axis=0)]:\n","        eegFeatureSet[n].fillna(eegFeatureSet[n].median(),inplace=True)\n","    \n","    #eegFeatureSet=eegFeatureSet.replace(np.nan,0)\n","\n","    #eegFeatureSet=np.log(eegFeatureSet)\n","    return eegFeatureSet"],"metadata":{"id":"R54O8iR33JxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def addFeatures(inpdata1, inpdata2, inpdata3):\n","    inpdata1['SubjectId']=(inpdata1.index.str.slice(0,1)).astype(int)\n","    inpdata1 = inpdata1.merge(inpdata2, on='SubjectId', how=\"inner\").set_axis(inpdata1.index)\n","    inpdata1['VideoLen']=inpdata3\n","    return inpdata1"],"metadata":{"id":"A6vI1T-44Wel"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dataScaling(inputdata1, inputdata2, kbestcols):\n","    '''assign best features from feature engineering. \n","    Split this data into train and test in 80:20 ratio.'''\n","\n","    #kbestcols=['Thetamean_60', 'Alpha1median_60', 'Thetamean_75', 'Thetamax_75']\n","    inputdata1=inputdata1[kbestcols]\n","\n","    y=inputdata2[[\"SubVdId\", \"UsrTgtLabel\"]].groupby(\"SubVdId\").first()\n","    Xtrain, Xtest, ytrain, ytest=train_test_split(inputdata1, y, test_size=0.2, random_state=randomState)\n","    \n","    idx=Xtrain.index.to_list()\n","    \n","    #cols=inpdata1.columns\n","\n","    #Creating the StandardScaler\n","    scaler = preprocessing.StandardScaler() #scaler=MinMaxScaler()\n","    output=pd.DataFrame(scaler.fit_transform(Xtrain))\n","\n","    output.columns=kbestcols\n","    output.index=idx\n","    \n","    #scaler=scaler.fit(inpdata1)\n","    #output=pd.DataFrame(scaler.transform(inpdata1))\n","    #output.columns=cols\n","\n","    #y=inpdata2[[\"SubVdId\", \"UsrTgtLabel\"]].groupby(\"SubVdId\").first()\n","    #output[\"TgtLabel\"]=y.UsrTgtLabel.to_list()\n","    #output.index=inpdata1.index.to_list()\n","\n","    return output, ytrain, scaler"],"metadata":{"id":"S0xQiq-v5lIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Randomly shuffle the train and test data\n","def splitTrainTest(input):\n","    l=list(range(0,100))\n","    s=input.index.unique()\n","    random.shuffle(l)\n","    indTrain, indTest=train_test_split(s[l],test_size=0.2,random_state=0)\n","\n","    indTrain=list(indTrain)\n","    indTest=list(indTest)\n","    return indTrain, indTest"],"metadata":{"id":"F8S4oNRW6aE0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def featureSelect1(input, noFeatures):\n","    model=ExtraTreesClassifier()\n","    model.fit(input.loc[indTrain, input.columns!='TgtLabel'],input.loc[indTrain, input.columns=='TgtLabel'].values.ravel())\n","\n","    featImportance=pd.Series(model.feature_importances_, index=input.loc[indTrain, input.columns!='TgtLabel'].columns)\n","    featImportance.nlargest(noFeatures).plot(kind='barh')\n","    plt.show()\n","    return featImportance.nlargest(noFeatures)"],"metadata":{"id":"QaV2kijq7xKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def featureSelect2(input, noFeatures):\n","    bestFeatures=SelectKBest(k=100)\n","\n","    fit=bestFeatures.fit(input.loc[indTrain, input.columns!='TgtLabel'],input.loc[indTrain, input.columns=='TgtLabel'].values.ravel())\n","    dfscores=pd.DataFrame(fit.scores_)\n","    dfcolumns=pd.DataFrame(input.loc[indTrain, input.columns!='TgtLabel'].columns)\n","\n","    featureScores=pd.concat([dfcolumns, dfscores],axis=1)\n","    featureScores.columns=['Cols','Scores']\n","\n","    kbestcols=featureScores.nlargest(noFeatures, 'Scores')['Cols']\n","    return kbestcols"],"metadata":{"id":"SSdqeLzY8oH4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def dataVisualize(input):\n","    pca=PCA(n_components=2)\n","    pca.fit(input.loc[indTrain, kbestcols]) #pca.fit(eegScaled.loc[:, kbestcols])\n","\n","    eegPCA=pca.transform(input.loc[indTrain, kbestcols]) #eegPCA=pca.transform(eegScaled.loc[:, kbestcols])\n","\n","    #eegScaled.shape, eegPCA.shape\n","\n","\n","    plt.figure(figsize=(8,8))\n","    plt.scatter(eegPCA[:,0], eegPCA[:,1], c=input.loc[indTrain, input.columns=='TgtLabel'].values.ravel()) #plt.scatter(eegPCA[:,0], eegPCA[:,1], c=y['TgtLabel'])\n","    plt.xlabel('First Principle Component')\n","    plt.ylabel('Second Principle Component')\n"],"metadata":{"id":"gFIrCOKG96IG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotCV(cvResults,xlab,ylab):\n","    plt.plot(cvResults['mean_test_score'], label='test')\n","    plt.plot(cvResults['mean_train_score'], label='train')\n","    plt.legend(loc='best')\n","    plt.ylim(ymax=1.0, ymin=0.0)\n","    plt.xlabel(xlab)\n","    plt.ylabel(ylab)\n","    plt.show()"],"metadata":{"id":"Hx2vTV44ZSBj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def modelCreate(classifier, param_grid, x, y):\n","    grid = GridSearchCV(classifier, param_grid, refit=True, verbose=0, scoring='accuracy', return_train_score=True)\n","    grid.fit(x,y)\n","\n","    model=grid.best_estimator_\n","    print(model)\n","\n","    return model"],"metadata":{"id":"QwFRi7asZ473"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotConfusionMatrix(predicted, actual):\n","    \n","    print(metrics.classification_report(actual, predicted))\n","    cm= plt.subplot()\n","    sns.heatmap(confusion_matrix(predicted,actual), annot=True,  ax=cm, cmap='PiYG')  \n","\n","    # Set the Title, Axis Labels and Class Labels\n","    cm.set_title('Confusion Matrix for Confusion Prediction')\n","    cm.set_xlabel('True Labels')\n","    cm.set_ylabel('Predicted Labels')\n","    cm.xaxis.set_ticklabels(['Not Confused', 'Confused']) \n","    cm.yaxis.set_ticklabels(['Not Confused', 'Confused'])\n"],"metadata":{"id":"F4d4tVVGWB6n"},"execution_count":null,"outputs":[]}]}